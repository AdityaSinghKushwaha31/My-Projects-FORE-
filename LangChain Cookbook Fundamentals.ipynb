{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359697d5",
   "metadata": {},
   "source": [
    "# LangChain Cookbook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d788b0",
   "metadata": {},
   "source": [
    "\n",
    "**Goal:** Provide an introductory understanding of the components and use cases of LangChain via [ELI5](https://www.dictionary.com/e/slang/eli5/#:~:text=ELI5%20is%20short%20for%20%E2%80%9CExplain,a%20complicated%20question%20or%20problem.) examples and code snippets. For use cases check out [part 2](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb). See [video tutorial](https://www.youtube.com/watch?v=2xxziIWmaSA) of this notebook.\n",
    "\n",
    "\n",
    "**Links:**\n",
    "* [LC Conceptual Documentation](https://docs.langchain.com/docs/)\n",
    "* [LC Python Documentation](https://python.langchain.com/en/latest/)\n",
    "* [LC Javascript/Typescript Documentation](https://js.langchain.com/docs/)\n",
    "* [LC Discord](https://discord.gg/6adMQxSpJS)\n",
    "* [www.langchain.com](https://langchain.com/)\n",
    "* [LC Twitter](https://twitter.com/LangChainAI)\n",
    "\n",
    "\n",
    "### **What is LangChain?**\n",
    "> LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "**~~TL~~DR**: LangChain makes the complicated parts of working & building with AI models easier. It helps do this in two ways:\n",
    "\n",
    "1. **Integration** - Bring external data, such as your files, other applications, and api data, to your LLMs\n",
    "2. **Agency** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next\n",
    "\n",
    "### **Why LangChain?**\n",
    "1. **Components** - LangChain makes it easy to swap out abstractions and components necessary to work with language models.\n",
    "\n",
    "2. **Customized Chains** - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
    "\n",
    "3. **Speed ðŸš¢** - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
    "\n",
    "4. **Community ðŸ‘¥** - Wonderful discord and community support, meet ups, hackathons, etc.\n",
    "\n",
    "Though LLMs can be straightforward (text-in, text-out) you'll quickly run into friction points that LangChain helps with once you develop more complicated applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9815081",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key=os.getenv('OPENAI_API_KEY', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be52ab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb564d",
   "metadata": {},
   "source": [
    "# LangChain Components\n",
    "\n",
    "## Schema - Nuts and Bolts of working with Large Language Models (LLMs)\n",
    "\n",
    "### **Text**\n",
    "The natural language way to interact with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e0dc06c",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What day comes after Friday?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You'll be working with simple strings (that'll soon grow in complexity!)\n",
    "my_text = \"What day comes after Friday?\"\n",
    "my_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39eb39",
   "metadata": {},
   "source": [
    "### **Chat Messages**\n",
    "Like text, but specified with a message type (System, Human, AI)\n",
    "\n",
    "* **System** - Helpful background context that tell the AI what to do\n",
    "* **Human** - Messages that are intented to represent the user\n",
    "* **AI** - Messages that show what the AI responded with\n",
    "\n",
    "For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b0935b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekansh Jain\\AppData\\Local\\Temp\\ipykernel_7112\\692300624.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "# This it the language model we'll use. We'll talk about what we're doing below in the next section\n",
    "chat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2f7af",
   "metadata": {},
   "source": [
    "Now let's create a few messages that simulate a chat experience with a bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "878d6a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You could try a Caprese salad with tomatoes, fresh mozzarella, basil, and a drizzle of balsamic glaze.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 39, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--f9341ad3-9fad-4980-b2b4-2e5db0f6c9b5-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
    "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a425aaa",
   "metadata": {},
   "source": [
    "You can also pass more chat history w/ responses from the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd3fe88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You should also explore the charming streets of the Old Town and indulge in delicious French cuisine.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "        AIMessage(content=\"You should go to Nice, France\"),\n",
    "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ee37a",
   "metadata": {},
   "source": [
    "You can also exclude the system message if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "238a49f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The day that comes after Thursday is Friday.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 13, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run--29343816-d4f3-4436-ac7e-bbaf65d1aef8-0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        HumanMessage(content=\"What day comes after Thursday?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf9634",
   "metadata": {},
   "source": [
    "### **Documents**\n",
    "An object that holds a piece of text and metadata (more information about that text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bbf58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ad9bef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd19754",
   "metadata": {},
   "source": [
    "But you don't have to include metadata if you don't want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0798d3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e462b5d",
   "metadata": {},
   "source": [
    "## Models - The interface to the AI brains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fe982",
   "metadata": {},
   "source": [
    "###  **Language Model**\n",
    "A model that does text in âž¡ï¸ text out!\n",
    "\n",
    "*Check out how I changed the model I was using from the default one to ada-001 (a very cheap, low performing model). See more models [here](https://platform.openai.com/docs/models)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b1a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekansh Jain\\AppData\\Local\\Temp\\ipykernel_7112\\954514504.py:3: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=openai_api_key)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6399c295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSaturday'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef89bfa",
   "metadata": {},
   "source": [
    "### **Chat Model**\n",
    "A model that takes a series of messages and returns a message output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf091777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekansh Jain\\AppData\\Local\\Temp\\ipykernel_13372\\3136461383.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4260711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekansh Jain\\AppData\\Local\\Temp\\ipykernel_13372\\1348698803.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chat(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the computer go to New York? To get a byte of the Big Apple!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 43, 'total_tokens': 61, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--32f82bc6-98e6-4504-8895-731164cbef08-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
    "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c028f9",
   "metadata": {},
   "source": [
    "### Function Calling Models\n",
    "\n",
    "[Function calling models](https://openai.com/blog/function-calling-and-other-api-updates) are similar to Chat Models but with a little extra flavor. They are fine tuned to give structured data outputs.\n",
    "\n",
    "This comes in handy when you're making an API call to an external service or doing extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1020ff45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"location\":\"Boston, MA\"}', 'name': 'get_current_weather'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 89, 'total_tokens': 106, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'function_call', 'logprobs': None}, id='run--81790ec0-6b02-49ee-8226-db84c705f7fb-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(model='gpt-4o-mini', temperature=1, openai_api_key=openai_api_key)\n",
    "\n",
    "output = chat(messages=\n",
    "     [\n",
    "         SystemMessage(content=\"You are an helpful AI bot\"),\n",
    "         HumanMessage(content=\"Whatâ€™s the weather like in Boston right now?\")\n",
    "     ],\n",
    "     functions=[{\n",
    "         \"name\": \"get_current_weather\",\n",
    "         \"description\": \"Get the current weather in a given location\",\n",
    "         \"parameters\": {\n",
    "             \"type\": \"object\",\n",
    "             \"properties\": {\n",
    "                 \"location\": {\n",
    "                     \"type\": \"string\",\n",
    "                     \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                 },\n",
    "                 \"unit\": {\n",
    "                     \"type\": \"string\",\n",
    "                     \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                 }\n",
    "             },\n",
    "             \"required\": [\"location\"]\n",
    "         }\n",
    "     }\n",
    "     ]\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f399a1d",
   "metadata": {},
   "source": [
    "See the extra `additional_kwargs` that is passed back to us? We can take that and pass it to an external API to get data. It saves the hassle of doing output parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b70f23",
   "metadata": {},
   "source": [
    "### **Text Embedding Model**\n",
    "Change your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Mainly used when comparing two pieces of text together.\n",
    "\n",
    "*BTW: Semantic means 'relating to meaning in language or logic.'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1655de82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekansh Jain\\AppData\\Local\\Temp\\ipykernel_7112\\3465051988.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2c85e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi! It's time for the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38bd816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc5a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample: [-0.00018827720297948732, -0.0030971646463529884, -0.0010296143488005749, -0.019197277531955735, -0.015154470537407164]...\n",
      "Your embedding is length 1536\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38fe99f",
   "metadata": {},
   "source": [
    "## Prompts - Text generally used as instructions to your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9318ed",
   "metadata": {},
   "source": [
    "### **Prompt**\n",
    "What you'll pass to the underlying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d270239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The statement is incorrect because there is a day missing between Monday and Wednesday, which is Tuesday.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "# I like to use three double quotation marks for my prompts because it's easier to read\n",
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74988254",
   "metadata": {},
   "source": [
    "### **Prompt Template**\n",
    "An object that helps create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
    "\n",
    "Think of it as an [f-string](https://realpython.com/python-f-strings/) in python but for prompts\n",
    "\n",
    "*Advanced: Check out LangSmithHub(https://smith.langchain.com/hub) for many more communit prompt templates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abcc212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I really want to travel to Rome. What should I do there?\n",
      "\n",
      "Respond in one short sentence\n",
      "\n",
      "-----------\n",
      "LLM Output: \n",
      "Explore the historic landmarks and try delicious Italian cuisine.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print (f\"Final Prompt: {final_prompt}\")\n",
    "print (\"-----------\")\n",
    "print (f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40bac2",
   "metadata": {},
   "source": [
    "### **Example Selectors**\n",
    "An easy way to select from a series of examples that allow you to dynamic place in-context information into your prompt. Often used when your task is nuanced or you have a large list of examples.\n",
    "\n",
    "Check out different types of example selectors [here](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)\n",
    "\n",
    "If you want an overview on why examples are important (prompt engineering), check out [this video](https://www.youtube.com/watch?v=dOxUroR57xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf36cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekansh Jain\\AppData\\Local\\Temp\\ipykernel_7572\\2663458360.py:7: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f57c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33aff469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.29-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.74 (from langchain-openai)\n",
      "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-openai) (1.93.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.4.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekansh jain\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai<2.0.0,>=1.86.0->langchain-openai) (0.4.6)\n",
      "Downloading langchain_openai-0.3.29-py3-none-any.whl (74 kB)\n",
      "Downloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
      "Installing collected packages: langchain-core, langchain-openai\n",
      "\n",
      "  Attempting uninstall: langchain-core\n",
      "\n",
      "    Found existing installation: langchain-core 0.3.71\n",
      "\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "    Uninstalling langchain-core-0.3.71:\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "      Successfully uninstalled langchain-core-0.3.71\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   ---------------------------------------- 0/2 [langchain-core]\n",
      "   -------------------- ------------------- 1/2 [langchain-openai]\n",
      "   -------------------- ------------------- 1/2 [langchain-openai]\n",
      "   -------------------- ------------------- 1/2 [langchain-openai]\n",
      "   -------------------- ------------------- 1/2 [langchain-openai]\n",
      "   ---------------------------------------- 2/2 [langchain-openai]\n",
      "\n",
      "Successfully installed langchain-core-0.3.74 langchain-openai-0.3.29\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfd0183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b4798b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,  # âœ… positional argument\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key),  # âœ… positional argument\n",
    "    Chroma,  # âœ… positional argument\n",
    "    k=2  # âœ… keyword argument\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67aceb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cf30107",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FewShotPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m similar_prompt = \u001b[43mFewShotPromptTemplate\u001b[49m(\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# The object that will help select examples\u001b[39;00m\n\u001b[32m      3\u001b[39m     example_selector=example_selector,\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Your prompt\u001b[39;00m\n\u001b[32m      6\u001b[39m     example_prompt=example_prompt,\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Customizations that will be added to the top and bottom of your prompt\u001b[39;00m\n\u001b[32m      9\u001b[39m     prefix=\u001b[33m\"\u001b[39m\u001b[33mGive the location an item is usually found in\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     suffix=\u001b[33m\"\u001b[39m\u001b[33mInput: \u001b[39m\u001b[38;5;132;01m{noun}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOutput:\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# What inputs your prompt will receive\u001b[39;00m\n\u001b[32m     13\u001b[39m     input_variables=[\u001b[33m\"\u001b[39m\u001b[33mnoun\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'FewShotPromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    \n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "369442bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: tree\n",
      "Example Output: ground\n",
      "\n",
      "Example Input: bird\n",
      "Example Output: nest\n",
      "\n",
      "Input: plant\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Select a noun!\n",
    "my_noun = \"plant\"\n",
    "# my_noun = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bb910f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' pot'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474c91d",
   "metadata": {},
   "source": [
    "### **Output Parsers Method 1: Prompt Instructions & String Parsing**\n",
    "A helpful way to format the output of a model. Usually used for structured output. LangChain has a bunch more output parsers listed on their [documentation](https://python.langchain.com/docs/modules/model_io/output_parsers).\n",
    "\n",
    "Two big concepts:\n",
    "\n",
    "**1. Format Instructions** - A autogenerated prompt that tells the LLM how to format it's response based off your desired result\n",
    "\n",
    "**2. Parser** - A method which will extract your model's text output into a desired structure (usually json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58353756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee36f881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekansh Jain\\AppData\\Local\\Temp\\ipykernel_13372\\623092675.py:1: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model_name=\"\", openai_api_key=openai_api_key)\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa59be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1079f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aaae5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc90f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "welcom to califonya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b116bb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"welcom to califonya!\", \\n\\t\"good_string\": \"Welcome to California!\"\\n}\\n```'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "985aa814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07045ae3",
   "metadata": {},
   "source": [
    "### **Output Parsers Method 2: OpenAI Fuctions**\n",
    "When OpenAI released function calling, the game changed. This is recommended method when starting out.\n",
    "\n",
    "They trained models specifically for outputing structured data. It became super easy to specify a Pydantic schema and get a structured output.\n",
    "\n",
    "There are many ways to define your schema, I prefer using Pydantic Models because of how organized they are. Feel free to reference OpenAI's [documention](https://platform.openai.com/docs/guides/gpt/function-calling) for other methods.\n",
    "\n",
    "In order to use this method you'll need to use a model that supports [function calling](https://openai.com/blog/function-calling-and-other-api-updates#:~:text=Developers%20can%20now%20describe%20functions%20to%20gpt%2D4%2D0613%20and%20gpt%2D3.5%2Dturbo%2D0613%2C). I'll use `gpt4-0613`\n",
    "\n",
    "**Example 1: Simple**\n",
    "\n",
    "Let's get started by defining a simple model for us to extract from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3593699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekansh Jain\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:3670: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Identifying information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The person's name\")\n",
    "    age: int = Field(..., description=\"The person's age\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The person's favorite food\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17033d15",
   "metadata": {},
   "source": [
    "Then let's create a chain (more on this later) that will do the extracting for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60b7be09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekansh Jain\\AppData\\Local\\Temp\\ipykernel_13372\\1152884003.py:26: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Sally' age=13 likes=None\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "# Define your output schema\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: Optional[int] = None\n",
    "    likes: Optional[str] = None\n",
    "\n",
    "# Define the prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"Extract structured information about people from the following text:\\n\\n{input}\"\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatOpenAI(model='gpt-4-0613', openai_api_key=openai_api_key)\n",
    "\n",
    "# Create the chain\n",
    "chain = create_structured_output_chain(Person, llm, prompt)\n",
    "\n",
    "# Run the chain\n",
    "result = chain.run(\n",
    "    \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37370210",
   "metadata": {},
   "source": [
    "Notice how we only have data on one person from that list? That is because we didn't specify we wanted multiple. Let's change our schema to specify that we want a list of people if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df4ad5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: Sequence[Person] = Field(..., description=\"The people in the text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bc127",
   "metadata": {},
   "source": [
    "Now we'll call for People rather than Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ba430d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ekansh Jain\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\json_schema.py:2324: PydanticJsonSchemaWarning: Default value default=Ellipsis description='The people in the text' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "People(people=[Person(name='Sally', age=13, likes=None), Person(name='Joey', age=12, likes='spinach'), Person(name='Caroline', age=23, likes=None)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = create_structured_output_chain(People, llm, prompt)\n",
    "chain.run(\n",
    "    \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db9b8b",
   "metadata": {},
   "source": [
    "Let's do some more parsing with it\n",
    "\n",
    "**Example 2: Enum**\n",
    "\n",
    "Now let's parse when a product from a list is mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6616a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-0613', openai_api_key=openai_api_key)\n",
    "\n",
    "class Product(str, enum.Enum):\n",
    "    CRM = \"CRM\"\n",
    "    VIDEO_EDITING = \"VIDEO_EDITING\"\n",
    "    HARDWARE = \"HARDWARE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5250ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Products(BaseModel):\n",
    "    \"\"\"Identifying products that were mentioned in a text\"\"\"\n",
    "\n",
    "    products: Sequence[Product] = Field(..., description=\"The products mentioned in a text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd7e0bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ekansh Jain\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\json_schema.py:2324: PydanticJsonSchemaWarning: Default value default=Ellipsis description='The products mentioned in a text' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Products(products=[<Product.CRM: 'CRM'>, <Product.HARDWARE: 'HARDWARE'>, <Product.VIDEO_EDITING: 'VIDEO_EDITING'>])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = create_structured_output_chain(Products, llm, prompt)\n",
    "chain.run(\n",
    "    \"The CRM in this demo is great. Love the hardware. The microphone is also cool. Love the video editing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43cec2",
   "metadata": {},
   "source": [
    "## Indexes - Structuring documents to LLMs can work with them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f904e9",
   "metadata": {},
   "source": [
    "### **Document Loaders**\n",
    "Easy ways to import data from other sources. Shared functionality with [OpenAI Plugins](https://openai.com/blog/chatgpt-plugins) [specifically retrieval plugins](https://github.com/openai/chatgpt-retrieval-plugin)\n",
    "\n",
    "See a [big list](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) of document loaders here. A bunch more on [Llama Index](https://llamahub.ai/) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4719d4",
   "metadata": {},
   "source": [
    "**HackerNews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f84a5726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (4.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba88e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee693520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://news.ycombinator.com/item?id=34422627\")\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e814f930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 comments\n",
      "Here's a sample:\n",
      "\n",
      "LangChain: Build AI apps with LLMs through composability | Hacker NewsHacker Newsnew | past | comments | ask | show | jobs | submitloginLangChain: Bui\n"
     ]
    }
   ],
   "source": [
    "print (f\"Found {len(data)} comments\")\n",
    "print (f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564583f",
   "metadata": {},
   "source": [
    "**Books from Gutenberg Project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72964fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GutenbergLoader\n",
    "\n",
    "loader = GutenbergLoader(\"https://www.gutenberg.org/cache/epub/2148/pg2148.txt\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47140a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o.â€”_Seneca_.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      At Paris, just after dark one gusty evening in the autumn of 18-,\n",
      "\n",
      "\n",
      "      I was enjoying the twofold luxury \n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content[1855:1990])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1386b0",
   "metadata": {},
   "source": [
    "**URLs and webpages**\n",
    "\n",
    "Let's try it out with [Paul Graham's website](http://www.paulgraham.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bab04f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Downloading unstructured-0.18.11-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (3.4.2)\n",
      "Collecting filetype (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured)\n",
      "  Downloading lxml-6.0.0-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting nltk (from unstructured)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (4.13.4)\n",
      "Collecting emoji (from unstructured)\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (0.6.7)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 981.5/981.5 kB 4.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (2.3.1)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Downloading rapidfuzz-3.13.0-cp313-cp313-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: backoff in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (4.14.0)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.42.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting wrapt (from unstructured)\n",
      "  Using cached wrapt-1.17.2-cp313-cp313-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\ekansh jain\\appdata\\roaming\\python\\python313\\site-packages (from unstructured) (7.0.0)\n",
      "Collecting python-oxmsg (from unstructured)\n",
      "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting html5lib (from unstructured)\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->unstructured) (2.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\ekansh jain\\appdata\\roaming\\python\\python313\\site-packages (from html5lib->unstructured) (1.17.0)\n",
      "Collecting webencodings (from html5lib->unstructured)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: click in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->unstructured) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->unstructured) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->unstructured) (2024.11.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekansh jain\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk->unstructured) (0.4.6)\n",
      "Collecting olefile (from python-oxmsg->unstructured)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->unstructured) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->unstructured) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->unstructured) (2025.6.15)\n",
      "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting cryptography>=3.1 (from unstructured-client->unstructured)\n",
      "  Downloading cryptography-45.0.6-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: httpcore>=1.0.9 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured) (1.0.9)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured) (2.11.7)\n",
      "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
      "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Collecting cffi>=1.14 (from cryptography>=3.1->unstructured-client->unstructured)\n",
      "  Using cached cffi-1.17.1-cp313-cp313-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.14->cryptography>=3.1->unstructured-client->unstructured)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore>=1.0.9->unstructured-client->unstructured) (0.16.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ekansh jain\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Downloading unstructured-0.18.11-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 5.6 MB/s eta 0:00:00\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 590.6/590.6 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading lxml-6.0.0-cp313-cp313-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 1.6/4.0 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.9/4.0 MB 7.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.4/4.0 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.9/4.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.9/4.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 3.5 MB/s eta 0:00:00\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
      "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp313-cp313-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.8/1.6 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.6/1.6 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading unstructured_client-0.42.2-py3-none-any.whl (207 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading cryptography-45.0.6-cp311-abi3-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 1.3/3.4 MB 7.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.9/3.4 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.1/3.4 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.4/3.4 MB 5.0 MB/s eta 0:00:00\n",
      "Using cached cffi-1.17.1-cp313-cp313-win_amd64.whl (182 kB)\n",
      "Downloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached wrapt-1.17.2-cp313-cp313-win_amd64.whl (38 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml): started\n",
      "  Building wheel for langdetect (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993363 sha256=1578657fda56041f447a1c1a2b9519d85d617fe24a14fac6161d264436c433fc\n",
      "  Stored in directory: c:\\users\\ekansh jain\\appdata\\local\\pip\\cache\\wheels\\eb\\87\\25\\2dddf1c94e1786054e25022ec5530bfed52bad86d882999c48\n",
      "Successfully built langdetect\n",
      "Installing collected packages: webencodings, filetype, wrapt, rapidfuzz, python-magic, python-iso639, pypdf, pycparser, olefile, lxml, langdetect, html5lib, emoji, aiofiles, python-oxmsg, nltk, cffi, cryptography, unstructured-client, unstructured\n",
      "\n",
      "   ----------------------------------------  0/20 [webencodings]\n",
      "   ----------------------------------------  0/20 [webencodings]\n",
      "   -- -------------------------------------  1/20 [filetype]\n",
      "   -- -------------------------------------  1/20 [filetype]\n",
      "   -- -------------------------------------  1/20 [filetype]\n",
      "   -- -------------------------------------  1/20 [filetype]\n",
      "   ---- -----------------------------------  2/20 [wrapt]\n",
      "   ---- -----------------------------------  2/20 [wrapt]\n",
      "   ---- -----------------------------------  2/20 [wrapt]\n",
      "   ------ ---------------------------------  3/20 [rapidfuzz]\n",
      "   ------ ---------------------------------  3/20 [rapidfuzz]\n",
      "   ------ ---------------------------------  3/20 [rapidfuzz]\n",
      "   ------ ---------------------------------  3/20 [rapidfuzz]\n",
      "   ------ ---------------------------------  3/20 [rapidfuzz]\n",
      "   ------ ---------------------------------  3/20 [rapidfuzz]\n",
      "   -------- -------------------------------  4/20 [python-magic]\n",
      "   -------- -------------------------------  4/20 [python-magic]\n",
      "   -------- -------------------------------  4/20 [python-magic]\n",
      "   ---------- -----------------------------  5/20 [python-iso639]\n",
      "   ---------- -----------------------------  5/20 [python-iso639]\n",
      "   ------------ ---------------------------  6/20 [pypdf]\n",
      "   ------------ ---------------------------  6/20 [pypdf]\n",
      "   ------------ ---------------------------  6/20 [pypdf]\n",
      "   ------------ ---------------------------  6/20 [pypdf]\n",
      "   ------------ ---------------------------  6/20 [pypdf]\n",
      "   ------------ ---------------------------  6/20 [pypdf]\n",
      "   ------------ ---------------------------  6/20 [pypdf]\n",
      "   ------------ ---------------------------  6/20 [pypdf]\n",
      "   ------------ ---------------------------  6/20 [pypdf]\n",
      "   ------------ ---------------------------  6/20 [pypdf]\n",
      "   ------------ ---------------------------  6/20 [pypdf]\n",
      "   -------------- -------------------------  7/20 [pycparser]\n",
      "   -------------- -------------------------  7/20 [pycparser]\n",
      "   -------------- -------------------------  7/20 [pycparser]\n",
      "   -------------- -------------------------  7/20 [pycparser]\n",
      "   -------------- -------------------------  7/20 [pycparser]\n",
      "   ---------------- -----------------------  8/20 [olefile]\n",
      "   ---------------- -----------------------  8/20 [olefile]\n",
      "   ------------------ ---------------------  9/20 [lxml]\n",
      "   ------------------ ---------------------  9/20 [lxml]\n",
      "   ------------------ ---------------------  9/20 [lxml]\n",
      "   ------------------ ---------------------  9/20 [lxml]\n",
      "   ------------------ ---------------------  9/20 [lxml]\n",
      "   ------------------ ---------------------  9/20 [lxml]\n",
      "   ------------------ ---------------------  9/20 [lxml]\n",
      "   ------------------ ---------------------  9/20 [lxml]\n",
      "   -------------------- ------------------- 10/20 [langdetect]\n",
      "   -------------------- ------------------- 10/20 [langdetect]\n",
      "   -------------------- ------------------- 10/20 [langdetect]\n",
      "   -------------------- ------------------- 10/20 [langdetect]\n",
      "   ---------------------- ----------------- 11/20 [html5lib]\n",
      "   ---------------------- ----------------- 11/20 [html5lib]\n",
      "   ---------------------- ----------------- 11/20 [html5lib]\n",
      "   ---------------------- ----------------- 11/20 [html5lib]\n",
      "   ---------------------- ----------------- 11/20 [html5lib]\n",
      "   ---------------------- ----------------- 11/20 [html5lib]\n",
      "   ---------------------- ----------------- 11/20 [html5lib]\n",
      "   ---------------------- ----------------- 11/20 [html5lib]\n",
      "   ---------------------- ----------------- 11/20 [html5lib]\n",
      "   ------------------------ --------------- 12/20 [emoji]\n",
      "   ------------------------ --------------- 12/20 [emoji]\n",
      "   ------------------------ --------------- 12/20 [emoji]\n",
      "   ------------------------ --------------- 12/20 [emoji]\n",
      "   ------------------------ --------------- 12/20 [emoji]\n",
      "   -------------------------- ------------- 13/20 [aiofiles]\n",
      "   -------------------------- ------------- 13/20 [aiofiles]\n",
      "   -------------------------- ------------- 13/20 [aiofiles]\n",
      "   ---------------------------- ----------- 14/20 [python-oxmsg]\n",
      "   ---------------------------- ----------- 14/20 [python-oxmsg]\n",
      "   ---------------------------- ----------- 14/20 [python-oxmsg]\n",
      "   ---------------------------- ----------- 14/20 [python-oxmsg]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   ------------------------------ --------- 15/20 [nltk]\n",
      "   -------------------------------- ------- 16/20 [cffi]\n",
      "   -------------------------------- ------- 16/20 [cffi]\n",
      "   -------------------------------- ------- 16/20 [cffi]\n",
      "   -------------------------------- ------- 16/20 [cffi]\n",
      "   -------------------------------- ------- 16/20 [cffi]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ---------------------------------- ----- 17/20 [cryptography]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   ------------------------------------ --- 18/20 [unstructured-client]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   -------------------------------------- - 19/20 [unstructured]\n",
      "   ---------------------------------------- 20/20 [unstructured]\n",
      "\n",
      "Successfully installed aiofiles-24.1.0 cffi-1.17.1 cryptography-45.0.6 emoji-2.14.1 filetype-1.2.0 html5lib-1.1 langdetect-1.0.9 lxml-6.0.0 nltk-3.9.1 olefile-0.47 pycparser-2.22 pypdf-5.9.0 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.13.0 unstructured-0.18.11 unstructured-client-0.42.2 webencodings-0.5.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46a54e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New: Good Writing | Founder Mode Want to start a startup? Get funded by Y Combinator . Â© mmxxv pg'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "urls = [\n",
    "    \"http://www.paulgraham.com/\",\n",
    "]\n",
    "\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9601db",
   "metadata": {},
   "source": [
    "### **Text Splitters**\n",
    "Often times your document is too long (like a book) for your LLM. You need to split it up into chunks. Text splitters help with this.\n",
    "\n",
    "There are many ways you could split your text into chunks, experiment with [different ones](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) to see which is best for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95713e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a54455f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n"
     ]
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "with open('C:\\\\Users\\\\Ekansh Jain\\\\Desktop\\\\Fore_School\\\\worked.txt') as f:\n",
    "    pg_work = f.read()\n",
    "    \n",
    "print (f\"You have {len([pg_work])} document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d19acb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pg_work])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3090f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 610 documents\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87a0f45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "February 2021Before college the two main things I worked on, outside of school,\n",
      "were writing and programming. I didn't write essays. I wrote what \n",
      "\n",
      "beginning writers were supposed to write then, and probably still\n",
      "are: short stories. My stories were awful. They had hardly any plot,\n"
     ]
    }
   ],
   "source": [
    "print (\"Preview:\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e670d",
   "metadata": {},
   "source": [
    "There are a ton of different ways to do text splitting and it really depends on your retrieval strategy and application design. Check out more splitters [here](https://python.langchain.com/docs/modules/data_connection/document_transformers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85defb",
   "metadata": {},
   "source": [
    "### **Retrievers**\n",
    "Easy way to combine documents with language models.\n",
    "\n",
    "There are many different types of retrievers, the most widely supported is the VectoreStoreRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8cccbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('C:\\\\Users\\\\Ekansh Jain\\\\Desktop\\\\Fore_School\\\\worked.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dab1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e62372be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0534bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7f8389169070>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3846a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db383cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standards; what was the point? No one else wanted one either, so\n",
      "off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would\n",
      "last.In this di\n",
      "\n",
      "much of it in grad school.Computer Science is an uneasy alliance between two halves, theory\n",
      "and systems. The theory people prove things, and the systems people\n",
      "build things. I wanted to build things. \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24193139",
   "metadata": {},
   "source": [
    "### **VectorStores**\n",
    "Databases to store vectors. Most popular ones are [Pinecone](https://www.pinecone.io/) & [Weaviate](https://weaviate.io/). More examples on OpenAIs [retriever documentation](https://github.com/openai/chatgpt-retrieval-plugin#choosing-a-vector-database). [Chroma](https://www.trychroma.com/) & [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) are easy to work with locally.\n",
    "\n",
    "Conceptually, think of them as tables w/ a column for embeddings (vectors) and a column for metadata.\n",
    "\n",
    "Example\n",
    "\n",
    "| Embedding      | Metadata |\n",
    "| ----------- | ----------- |\n",
    "| [-0.00015641732898075134, -0.003165106289088726, ...]      | {'date' : '1/2/23}       |\n",
    "| [-0.00035465431654651654, 1.4654131651654516546, ...]   | {'date' : '1/3/23}        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c5533ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "661fdf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 78 documents\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e99ac0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89e7758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 78 embeddings\n",
      "Here's a sample of one: [-0.001058628615053026, -0.01118234211553424, -0.012874804746266883]...\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(embedding_list)} embeddings\")\n",
    "print (f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac358c5",
   "metadata": {},
   "source": [
    "Your vectorstore store your embeddings (â˜ï¸) and make them easily searchable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9b79b",
   "metadata": {},
   "source": [
    "## Memory\n",
    "Helping LLMs remember information.\n",
    "\n",
    "Memory is a bit of a loose term. It could be as simple as remembering information you've chatted about in the past or more complicated information retrieval.\n",
    "\n",
    "We'll keep it towards the Chat Message use case. This would be used for chat bots.\n",
    "\n",
    "There are many types of memory, explore [the documentation](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html) to see which one fits your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b49da",
   "metadata": {},
   "source": [
    "### Chat Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "893a18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "history.add_user_message(\"what is the capital of france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2949fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!'),\n",
       " HumanMessage(content='what is the capital of france?')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b74d5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chat(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "529e168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!'),\n",
       " HumanMessage(content='what is the capital of france?'),\n",
       " AIMessage(content='The capital of France is Paris.')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_ai_message(ai_response.content)\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fc79c",
   "metadata": {},
   "source": [
    "## Chains â›“ï¸â›“ï¸â›“ï¸\n",
    "Combining different LLM calls and action automatically\n",
    "\n",
    "Ex: Summary #1, Summary #2, Summary #3 > Final Summary\n",
    "\n",
    "Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo&t=2s) explaining different summarization chain types\n",
    "\n",
    "There are [many applications of chains](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) search to see which are best for your use case.\n",
    "\n",
    "We'll cover two of them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ba415",
   "metadata": {},
   "source": [
    "### 1. Simple Sequential Chains\n",
    "\n",
    "Easy chains where you can use the output of an LLM as an input into another. Good for breaking up tasks (and keeping your LLM focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "79fc0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43d4494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b6c8e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e0b83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d19c64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "A classic dish from Rome is Spaghetti alla Carbonara, featuring egg, Parmesan cheese, black pepper, and pancetta or guanciale.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Ingredients:\n",
      "- 8oz spaghetti \n",
      "- 4 tablespoons olive oil\n",
      "- 4oz diced pancetta or guanciale\n",
      "- 2 cloves garlic, minced\n",
      "- 2 eggs, lightly beaten\n",
      "- 2 tablespoons parsley, chopped \n",
      "- Â½ cup grated Parmesan \n",
      "- Salt and black pepper to taste\n",
      "\n",
      "Instructions:\n",
      "1. Bring a pot of salted water to a boil and add the spaghetti. Cook according to package directions. \n",
      "2. Meanwhile, add the olive oil to a large skillet over medium-high heat. Add the diced pancetta and garlic, and cook until pancetta is browned and garlic is fragrant.\n",
      "3. In a medium bowl, whisk together the eggs, parsley, Parmesan, and salt and pepper.\n",
      "4. Drain the cooked spaghetti and add it to the skillet with the pancetta and garlic. Remove from heat and pour the egg mixture over the spaghetti, stirring to combine. \n",
      "5. Serve the spaghetti alla carbonara with additional Parmesan cheese and black pepper.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6191bf5",
   "metadata": {},
   "source": [
    "### 2. Summarization Chain\n",
    "\n",
    "Easily run through long numerous documents and get a summary. Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo) for other chain types besides map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6f218c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"January 2017Because biographies of famous scientists tend to \n",
      "edit out their mistakes, we underestimate the \n",
      "degree of risk they were willing to take.\n",
      "And because anything a famous scientist did that\n",
      "wasn't a mistake has probably now become the\n",
      "conventional wisdom, those choices don't\n",
      "seem risky either.Biographies of Newton, for example, understandably focus\n",
      "more on physics than alchemy or theology.\n",
      "The impression we get is that his unerring judgment\n",
      "led him straight to truths no one else had noticed.\n",
      "How to explain all the time he spent on alchemy\n",
      "and theology?  Well, smart people are often kind of\n",
      "crazy.But maybe there is a simpler explanation. Maybe\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"the smartness and the craziness were not as separate\n",
      "as we think. Physics seems to us a promising thing\n",
      "to work on, and alchemy and theology obvious wastes\n",
      "of time. But that's because we know how things\n",
      "turned out. In Newton's day the three problems \n",
      "seemed roughly equally promising. No one knew yet\n",
      "what the payoff would be for inventing what we\n",
      "now call physics; if they had, more people would \n",
      "have been working on it. And alchemy and theology\n",
      "were still then in the category Marc Andreessen would \n",
      "describe as \"huge, if true.\"Newton made three bets. One of them worked. But \n",
      "they were all risky.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" Biographies of famous scientists often edit out their mistakes, giving readers the wrong impression that they never faced any risks to achieve successful results. An example of this is Newton, whose smartness is assumed to have led straight him to truths without any detours into alchemy or theology - despite the fact that he spent a lot of time on both fields. Maybe the simpler explanation is that he was willing to take risks, even if it means potentially making mistakes.\n",
      "\n",
      " In the 17th century, Newton took a risk and made three bets, one of which turned out to be a successful invention of what we now call physics. The other two bets were on less popular subjects of the time such as alchemy and theology. People did not know then what the payoff would be, but the bets still seemed relatively promising.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Biographies tend to omit famous scientists' mistakes from their stories, but Newton was willing to take risks and explore multiple fields to make his discoveries. He placed three risky bets, one of which resulted in the creation of physics as we know it today.\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f6193c",
   "metadata": {},
   "source": [
    "## Agents ðŸ¤–ðŸ¤–\n",
    "\n",
    "Official LangChain Documentation describes agents perfectly (emphasis mine):\n",
    "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an **unknown chain** that depends on the user's input. In these types of chains, there is a â€œagentâ€ which has access to a suite of tools. Depending on the user input, the agent can then **decide which, if any, of these tools to call**.\n",
    "\n",
    "\n",
    "Basically you use the LLM not just for text output, but also for decision making. The coolness and power of this functionality can't be overstated enough.\n",
    "\n",
    "Sam Altman emphasizes that the LLMs are good '[reasoning engine](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'. Agent take advantage of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce05d51",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "The language model that drives decision making.\n",
    "\n",
    "More specifically, an agent takes in an input and returns a response corresponding to an action to take along with an action input. You can see different types of agents (which are better for different use cases) [here](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696b65c",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "A 'capability' of an agent. This is an abstraction on top of a function that makes it easy for LLMs (and agents) to interact with it. Ex: Google search.\n",
    "\n",
    "This area shares commonalities with [OpenAI plugins](https://platform.openai.com/docs/plugins/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f8231",
   "metadata": {},
   "source": [
    "### Toolkit\n",
    "\n",
    "Groups of tools that your agent can select from\n",
    "\n",
    "Let's bring them all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67d5d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ddcdbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "serpapi_api_key=os.getenv(\"SERP_API_KEY\", \"YourAPIKey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44fad67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=serpapi_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f544a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4882754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should try to find out what band Natalie Bergman is a part of.\n",
      "Action: Search\n",
      "Action Input: \"Natalie Bergman band\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Natalie Bergman is an American singer-songwriter. She is one half of the duo Wild Belle, along with her brother Elliot Bergman. Her debut solo album, Mercy, was released on Third Man Records on May 7, 2021. She is based in Los Angeles.', 'Natalie Bergman type: American singer-songwriter.', 'Natalie Bergman main_tab_text: Overview.', 'Natalie Bergman kgmid: /m/0qgx4kh.', 'Natalie Bergman genre: Folk.', 'Natalie Bergman parents: Susan Bergman, Judson Bergman.', 'Natalie Bergman born: 1988 or 1989 (age 34â€“35).', 'Natalie Bergman is an American singer-songwriter. She is one half of the duo Wild Belle, along with her brother Elliot Bergman. Her debut solo album, Mercy, ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should search for the first album of Wild Belle\n",
      "Action: Search\n",
      "Action Input: \"Wild Belle first album\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIsles\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Isles is the first album of the band that Natalie Bergman is a part of.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent({\"input\":\"what was the first album of the\" \n",
    "                    \"band that Natalie Bergman is a part of?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c30d2",
   "metadata": {},
   "source": [
    "![Wild Belle](data/WildBelle1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4b368",
   "metadata": {},
   "source": [
    "ðŸŽµEnjoyðŸŽµ\n",
    "https://open.spotify.com/track/1eREJIBdqeCcqNCB1pbz7w?si=c014293b63c7478c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
